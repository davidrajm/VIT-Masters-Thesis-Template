
\chapter{System Design}
The research problem can be expressed using fundamental equations in data science. One of the key formulations in machine learning is the cost function for linear regression:
\begin{equation}
	J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta (x^{(i)}) - y^{(i)} \right)^2
\end{equation}
where:
\begin{itemize}
	\item $J(\theta)$ is the cost function,
	\item $h_\theta(x^{(i)})$ is the hypothesis function,
	\item $y^{(i)}$ is the actual value,
	\item $m$ is the number of training examples.
\end{itemize}

Another fundamental concept in deep learning is the softmax function used in classification:
\begin{equation}
	\sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}}
\end{equation}
where $z$ is the input vector and $K$ is the number of classes.

Matrices play a crucial role in data science, especially in neural networks. A typical forward propagation step in a neural network can be expressed as:
\begin{equation}
	A^{[l]} = g(W^{[l]} A^{[l-1]} + b^{[l]})
\end{equation}
where:
\begin{description}
	\item $A^{[l]}$ is the activation at layer $l$,
	\item $W^{[l]}$ is the weight matrix,
	\item $b^{[l]}$ is the bias vector,
	\item $g(\cdot)$ is the activation function.
\end{description}

An example of a weight matrix in a neural network with three inputs and two neurons is:
\begin{equation}
	W = \begin{bmatrix} 0.2 & 0.4 & 0.6 \\ 0.8 & 1.0 & 1.2 \end{bmatrix}
\end{equation}

\section{Typing theorems and definitions}
\begin{definition}[Probability Distribution]
	A probability distribution is a function that provides the probabilities of occurrence of different possible outcomes in an experiment. Formally, for a discrete random variable $X$, the probability mass function (PMF) is defined as:
	\begin{equation}
		P(X = x) = f(x), \quad \sum_{x} f(x) = 1.
	\end{equation}
\end{definition}

\begin{lemma}[Universal Approximation Theorem]
	A feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of $\mathbb{R}^n$, provided the activation function is non-constant, bounded, and monotonically increasing.
\end{lemma}

\begin{theorem}[Central Limit Theorem]
	Let $X_1, X_2, ..., X_n$ be a sequence of independent and identically distributed random variables with mean $\mu$ and variance $\sigma^2$. Then, as $n$ approaches infinity, the standardized sum:
	\begin{equation}
		Z_n = \frac{\sum_{i=1}^{n} X_i - n\mu}{\sigma\sqrt{n}}
	\end{equation}
	converges in distribution to the standard normal distribution:
	\begin{equation}
		Z_n \sim \mathcal{N}(0,1).
	\end{equation}
\end{theorem}

If you want more such environments to be numbered and formatted consistently, add the following code in the preamble of your document and replace the commands, what you need and use it as environments.

\begin{verbatim}
\newtheorem{corollary}[theorem]{Corollary}
\end{verbatim}
Note that, this follows the numbering of theorem environment. 

\begin{remark}
This remark is typed using the similar theorem environment but without numbering. Note that, we have added the following in the preamble to achieve this:
\begin{verbatim}
\newtheorem{remark}{Remark}
\end{verbatim}
\end{remark}

